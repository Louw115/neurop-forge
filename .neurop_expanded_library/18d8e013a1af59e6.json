{
  "composition": {
    "composable": true,
    "composition_notes": "Atomic string block, composable with standard types",
    "conflicts_with": [],
    "input_compatible_types": [
      "arithmetic",
      "string",
      "collection",
      "utility"
    ],
    "output_compatible_types": [
      "arithmetic",
      "string",
      "collection",
      "validation",
      "utility"
    ],
    "requires_blocks": []
  },
  "constraints": {
    "deterministic": true,
    "io_operations": [
      "none"
    ],
    "max_execution_time_ms": null,
    "max_memory_bytes": null,
    "purity": "pure",
    "side_effects": [],
    "thread_safe": true
  },
  "failure_modes": {
    "can_fail": false,
    "error_conditions": [
      "Invalid input types",
      "None values when not allowed"
    ],
    "failure_mode": "never",
    "possible_exceptions": [
      "Exception"
    ],
    "recovery_hints": [
      "Validate inputs before calling",
      "Handle exceptions appropriately"
    ]
  },
  "identity": {
    "algorithm": "sha256",
    "hash_value": "18d8e013a1af59e696bf9892b6adb1c5a01d1304ce9fe54a2cb3ae4fbe15064b",
    "semantic_fingerprint": "501285e005e0b95d894d5c017c140549402822953f351cb6e05d5288164ea6d6",
    "version": "1.0.0"
  },
  "interface": {
    "description": "Tokenize a search query.",
    "inputs": [
      {
        "data_type": "string",
        "default_value": null,
        "description": "Parameter query",
        "name": "query",
        "optional": false
      }
    ],
    "outputs": [
      {
        "data_type": "list",
        "default_value": null,
        "description": "Return value of tokenize_query",
        "name": "result",
        "optional": false
      }
    ]
  },
  "logic": "def tokenize_query(query: str) -> list:\n    \"\"\"Tokenize a search query.\"\"\"\n    return [token.strip().lower() for token in query.split() if token.strip()]",
  "metadata": {
    "category": "string",
    "created_at": "2026-01-10T12:22:28.509475+00:00",
    "description": "Tokenize a search query.",
    "intent": "Tokenize a search query.",
    "language": "python",
    "name": "tokenize_query",
    "source_file": "neurop_forge/sources/search_patterns.py",
    "source_line_end": 14,
    "source_line_start": 12,
    "tags": [
      "query",
      "string_manipulation",
      "tokenize"
    ],
    "version": "1.0.0"
  },
  "ownership": {
    "attribution_required": true,
    "license_type": "MIT",
    "license_url": null,
    "modifications_allowed": true,
    "original_author": "Lourens Wasserman",
    "original_repository": "neurop-block-forge"
  },
  "sealed": true,
  "trust_score": {
    "determinism_score": 1.0,
    "execution_count": 0,
    "failure_count": 0,
    "last_verified": "2026-01-10T12:22:28.509495+00:00",
    "license_score": 1.0,
    "overall_score": 0.385,
    "risk_factors": [],
    "risk_level": "low",
    "static_analysis_score": 0.9,
    "success_count": 0,
    "success_rate": 0.0,
    "test_coverage_score": 0.0
  },
  "validation_rules": {
    "input_validators": [
      "type(query) == string",
      "query is not None"
    ],
    "invariants": [],
    "output_validators": [
      "type(result) == list"
    ],
    "postconditions": [
      "type(result) == list"
    ],
    "preconditions": [
      "type(query) == string",
      "query is not None"
    ]
  }
}